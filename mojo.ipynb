{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "mojoYearStarter = 'https://www.boxofficemojo.com/year/'\n",
    "mojoBoxStarter = 'https://www.boxofficemojo.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-repository",
   "metadata": {},
   "source": [
    "<h2>Obtain links to all movies for the year</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "def find_movie_links(year):\n",
    "    '''\n",
    "    The link mojoYearStarter leads to a page containing links to the 200 top domestic earning movies for the year.\n",
    "    This function returns a list of those 200 links.\n",
    "    '''\n",
    "    page = requests.get(mojoYearStarter + year)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser') # Create a beautiful soup object\n",
    "    table = soup.find('table') # Find all links inside the year's table\n",
    "    links_list = [i.get('href') for i in table.find_all(class_='a-link-normal') if i.get('href').startswith('/release')] # Find a list of all links that lead to movies\n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-possible",
   "metadata": {},
   "source": [
    "<h2>Obtain all movie-specific attributes on a movie webpage</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(soup):\n",
    "    '''\n",
    "    Each webpage for a movie on Box Office Mojo contains various attributes that we want to store.\n",
    "    This function returns a dictionary of several movie-specific attributes found on the page.\n",
    "    '''\n",
    "    \n",
    "    attributes = {\n",
    "        'Title':None,\n",
    "        'Domestic':None, \n",
    "        'International':None, \n",
    "        'Budget':None,\n",
    "        'Distributor':None,\n",
    "        'MPAA-Rating':None,\n",
    "        'Runtime':None,\n",
    "        'Genres':None\n",
    "    }    \n",
    "    \n",
    "    attributes['Title'] = soup.find('h1').get_text() # Find title\n",
    "    \n",
    "    \n",
    "    money = soup.find_all(class_='money') # Find domestic and international box office numbers\n",
    "    attributes['Domestic'], attributes['International'] = money[0].get_text(), money[1].get_text()\n",
    "    \n",
    "    atts = soup.find(class_='a-section a-spacing-none mojo-summary-values mojo-hidden-from-mobile').find_all(class_='a-section a-spacing-none') # gets information from the table to right of Grosses\n",
    "\n",
    "    for a in atts: # Iterate through each field in the attributes table\n",
    "        field_name = list(a.children)[0].get_text() # Find the name of the current field iteration\n",
    "        try:\n",
    "            if(field_name == 'Distributor'):\n",
    "                attributes['Distributor'] = str(list(list(a.children)[1].children)[0]) # Find Distributor\n",
    "            elif(field_name == 'Budget'):\n",
    "                attributes['Budget'] = list(a.children)[1].get_text() # Find Budget\n",
    "            elif(field_name == 'MPAA'):\n",
    "                attributes['MPAA-Rating'] = list(a.children)[1].get_text() # Find Rating\n",
    "            elif(field_name == 'Running Time'):\n",
    "                attributes['Runtime'] = list(a.children)[1].get_text() # Find Runtime\n",
    "            elif(field_name == 'Genres'):\n",
    "                attributes['Genres'] = ';'.join(list(a.children)[1].get_text().replace('\\n', '').split()) # Find a string of genres and separate them by semicolon\n",
    "        except:\n",
    "            pass # If this logic is not able to select a valid field entry, then it does not exist, so we keep None for the entry \n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-binding",
   "metadata": {},
   "source": [
    "<h2>Check if the Daily tab is disabled for a particular movie</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_tab_disabled(soup):\n",
    "    '''\n",
    "    Some movie webpages do not have Daily Data, they only contain Weekly Data. We skip these movies.\n",
    "    This function returns True if the Daily Data table is disabled for the movie.\n",
    "    '''\n",
    "    string = list(soup.find_all(class_='mojo-tab-container')[2].children)[1].get('class')[-1] # Find if daily is disabled (str 'mojo-disabled-tab')\n",
    "    return string == 'mojo-disabled-tab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-skiing",
   "metadata": {},
   "source": [
    "<h2>Find and manipulate the Daily Data table on a movie webpage</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_table(soup):\n",
    "    '''\n",
    "    The Daily Data we want from each movie webpage is the only html table on the page. \n",
    "    For simplicity, we only care about copying data that can not be created using other data already available.\n",
    "    This function returns the Daily Data table as a Pandas DataFrame.\n",
    "    '''\n",
    "    table = pd.read_html(str(soup.find('table')))[0] # Find table on the page\n",
    "    table = table[['Date', 'Daily', 'Theaters', 'Rank']] # Take base daily information from the table\n",
    "    table['Date'] = table['Date'].apply(lambda s: s[:11] if s[5] == ',' else s[:12]) # Remove holiday description from the string\n",
    "    table['Date'] = pd.to_datetime(table['Date']) # Change column to datetime\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-celtic",
   "metadata": {},
   "source": [
    "<h2>Collect and save Daily Data and Attribute Data for every movie from 2000-2020</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script runs iterates through each year and collects data, then saves it before beginning the next year. \n",
    "Also collects metadata about how many movies had disabled Daily Data and how long it took to scrape each webpage.\n",
    "\n",
    "The original data storage is a dictionary creatively named \"items\" whose keys are Movie titles (+ the year they were released)\n",
    "    and whose values are a 2-element tuple containing a dictionary of the movie-specific attributes and a Pandas DataFrame\n",
    "    containing the movie's Daily Data. Requires an initial pickled dictionary stored at \"data_pickles/items.p\".  \n",
    "'''\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "for year in range(2000, 2021):\n",
    "    links_list = find_movie_links(str(year)) # Iterate through each webpage using the year\n",
    "    items = pickle.load( open( \"data_pickles/items.p\", \"rb\" ) )\n",
    "    disabled_count = 0 # Keep track of how many movies had disabled daily tabs\n",
    "    movie_already_added_count = 0 # Keep track of how many times a movie was already added to items: ended up being recorded incorrectly because I re-recorded some years, so this number is inflated. Originally intended to capture how many movies were already recorded from the previous year \n",
    "    times = [] # Keep track of the times to graph distribution of scrape time per movie\n",
    "    for idx, link in enumerate(links_list):\n",
    "\n",
    "        timeStart = time.time()\n",
    "\n",
    "        page = requests.get(mojoBoxStarter + link)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        if(daily_tab_disabled(soup)):\n",
    "            disabled_count += 1\n",
    "            timeStart = time.time() # reset timer\n",
    "            print('skipped a movie: {}'.format(mojoBoxStarter + link))\n",
    "            continue # Skip this link if the daily tab is disabled\n",
    "\n",
    "        table = get_table(soup) # Retrieve daily data DataFrame for the movie\n",
    "        attributes = get_attributes(soup) # Retrieve attributes dictionary for the movie\n",
    "        attributes['Title'] = attributes['Title'] + ' ({})'.format(table.Date[0].year) # Add the release year (the first date entry) to title to not confuse with duplicate titles from previous years\n",
    "\n",
    "        if(attributes['Title']) in items:\n",
    "            movie_already_added_count += 1\n",
    "            timeStart = time.time()\n",
    "            print('Movie already stored: {}'.format(attributes['Title']))\n",
    "            continue # Skip adding this movie to our items dictionary if it already exists from the previous year's search\n",
    "\n",
    "        items[attributes['Title']] = (attributes, table) # Add the (attributes dict, daily data DataFrame) tuple to the items dictionary\n",
    "\n",
    "        end_time = time.time() - timeStart\n",
    "        times.append(end_time)\n",
    "        print('Stored {} in {} seconds, item {} of {}'.format(attributes['Title'], end_time, idx + 1, len(links_list)))\n",
    "\n",
    "    pickle.dump(items, open( \"data_pickles/items.p\",  \"wb\" ) ) # Save the pickle!\n",
    "    pickle.dump((disabled_count, movie_already_added_count, times), open( \"metadata/{}.p\".format(year), \"wb\" ) ) # Store tuple of counts and times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-tuesday",
   "metadata": {},
   "source": [
    "<h2> This is the code for creating the final Daily Data and Attribute DataFrames from items object </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script splits the items dictionary into separate DataFrames for the Daily Data and movie-specific Attributes.\n",
    "The Daily Data DataFrame is 2-indexed by Movie Title (+ year) and Date while the Attributes DataFrame is indexed by Title (+ year).\n",
    "\n",
    "'''\n",
    "daily_df = pd.DataFrame({})\n",
    "attribute_df = pd.DataFrame({})\n",
    "for movie, value in items.items(): # Iterate through stored movies and concatenate them\n",
    "    value[1]['Movie_Title'] = movie # Add the movie title column to the daily data DataFrame so they can be indexed later\n",
    "    daily_df = pd.concat([daily_df, value[1]])\n",
    "    attribute_df = pd.concat([attribute_df, pd.DataFrame(value[0], index = [0])]) # Attribute is stored as dict, so we create a DataFrame to concatenate with\n",
    "    \n",
    "\n",
    "daily_df.set_index(['Movie_Title', 'Date'], inplace = True)\n",
    "attribute_df.set_index('Title', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.to_pickle(\"data_pickles/Daily_DataFrame.p\") # More pickle saving!\n",
    "attribute_df.to_pickle(\"data_pickles/Attributes_DataFrame.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-annual",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
